# from abc import abstractmethod
import tensorflow as tf
import tensorflow_datasets as tfds
import math
import warnings
import numpy as np
from sklearn.utils import shuffle as sklearn_shuffle
from tqdm import tqdm
import os
from .utils import convert_to_human_readable
import termcolor
import json
import multiprocessing
from functools import partial


class Writer(object):
    def __init__(
        self, source_directory, destination_directory, config=None, version="1.0.0"
    ):
        self.source_directory = source_directory
        self.destination_directory = destination_directory

        if not os.path.exists(self.destination_directory):
            os.mkdir(self.destination_directory)

        self.version = version

        if config is None:
            config = self.destination_directory.split("/")[-1]
        self.config = config

        if not os.path.exists(os.path.join(self.destination_directory, self.config)):
            os.mkdir(os.path.join(self.destination_directory, self.config))

        if not os.path.exists(
            os.path.join(self.destination_directory, self.config, self.version)
        ):
            os.mkdir(
                os.path.join(self.destination_directory, self.config, self.version)
            )

    def extended_dataset_info(self):
        return None

    @property
    def _extended_dataset_info(self):
        extended_info = self.extended_dataset_info()
        if extended_info is not None:
            # Verify extended data is json serializable
            try:
                json.dumps(extended_info)
            except TypeError as e:
                print(f"Extended dataset info is not json serializable.  Error: {e}")

        return extended_info

    def _write_extended_dataset_info(self):
        if self._extended_dataset_info is not None:
            extended_dataset_info_path = os.path.join(
                self.destination_directory,
                self.config,
                self.version,
                "extended_dataset_info.json",
            )
            # Write extended dataset info to json
            with open(extended_dataset_info_path, "w") as f:
                json.dump(self._extended_dataset_info, f, indent=4)

    def extend_meta_data(self):
        description = ""
        homepage = ""
        supervised_keys = None
        citation = ""

        return description, homepage, supervised_keys, citation

    def _write_meta_data(self, split_infos):
        (
            description,
            homepage,
            supervised_keys,
            citation,
        ) = self.extend_meta_data()

        tfds.folder_dataset.write_metadata(
            data_dir=os.path.join(
                self.destination_directory, self.config, self.version
            ),
            features=self._features,
            split_infos=split_infos,
            filename_template="{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
            description=description,
            homepage=homepage,
            version=self.version,
            supervised_keys=supervised_keys,
            citation=citation,
        )

    def features(self):
        """Define entries within a single TFRecord example proto

        Assumes the output will be serialized with .SerializeToString()
        """
        raise NotImplementedError(f"features must be defined when subclassing Writer")

    @property
    def _features(self):
        return self.features()

    def process_data(self, example):
        """Processing to clean the data before converting the data over to a TFRecord
        example.  For instance, process_data(filename) would accept a file name, load
        the data from the file, perform requisite processing, and return the data in the
        desired saving format.

        If train_info, val_info, or test_info are passed to `write_records`,
        `process_data` will operate under a single example from the info passed to
        `write_records`.

        ***Must return a named dictionary (so that _estimate_mb_per_example will work)
        """
        raise NotImplementedError(
            f"process_data must be defined when subclassing Writer"
        )

    def _process_data(self, example):
        processed_data = self.process_data(example)
        # print(self._features.to_json_content())
        return self._features.serialize_example(processed_data)

    def _estimate_bytes_per_example(self, info, n_estimates, verbose=0):
        if verbose > 0:
            print("Estimating bytes per example...")

        total_bytes = 0

        for instance in info[:n_estimates]:
            # processed_data = self.process_data(instance)
            # serialized_data = self._features.serialize_example(processed_data)
            serialized_data = self._process_data(instance)
            total_bytes += len(serialized_data)

        avg_bytes_per_example = total_bytes / n_estimates
        return avg_bytes_per_example

    def _estimate_n_examples_per_shard(
        self, mb_per_shard, info, n_estimates, verbose=0
    ):
        avg_bytes_per_example = self._estimate_bytes_per_example(
            info, n_estimates, verbose=verbose
        )
        byte_per_shard = mb_per_shard * (1024**2)
        n_examples_per_shard = math.ceil((1 / avg_bytes_per_example) * byte_per_shard)
        return n_examples_per_shard

    def _write_shard(self, shard_info_and_name, verbose=0):
        shard_info, shard_name = shard_info_and_name

        # Writes a single shard to a file
        if verbose >= 3:
            shard_pbar = tqdm(
                shard_info, desc=f"Writing shard {shard_name}", leave=False
            )
        else:
            shard_pbar = shard_info

        shard_name = os.path.join(
            self.destination_directory, self.config, self.version, shard_name
        )
        with tf.io.TFRecordWriter(shard_name) as out:
            for example in shard_pbar:
                # processed_data = self.process_data(example)
                # serialized_data = self._features.serialize_example(processed_data)
                serialized_data = self._process_data(example)
                out.write(serialized_data)

        return os.path.getsize(shard_name)

    def _write_shards(self, splits_shards, verbose=0, n_workers=1):
        if verbose > 0:
            split_pbar = tqdm(splits_shards, desc="Writing splits")  # 1, ...,
        else:
            split_pbar = splits_shards

        split_infos = []

        for split in split_pbar:
            split_name = split["name"]
            num_shards = len(split["shards"])
            dest_base = self.destination_directory.split("/")[-1]
            shard_names = [
                f"{dest_base}-{split_name}.tfrecord-{i:05d}-of-{num_shards:05d}"
                for i in range(len(split["shards"]))
            ]

            if verbose >= 2:
                shard_pbar = tqdm(
                    zip(split["shards"], shard_names),
                    desc=f"Writing {split_name} shards",
                    total=len(split["shards"]),
                    leave=False,
                )
            else:
                shard_pbar = zip(split["shards"], shard_names)

            split_bytes = 0

            if n_workers > 1:
                verbose = min(verbose, 2)
                # write_shard = lambda shard, shard_name: self._write_shard(
                #     shard, shard_name, verbose=verbose
                # )

                # Create a partial function with _write_shard and verbose arguments
                partial_write_shard = partial(self._write_shard, verbose=verbose)

                with multiprocessing.Pool(processes=n_workers) as pool:
                    for bytes in pool.imap_unordered(
                        partial_write_shard, shard_pbar, chunksize=n_workers
                    ):
                        split_bytes += bytes
                        shard_pbar.update(1)

            else:
                for shard_info, shard_name in shard_pbar:
                    # Get the shard name
                    # dest_base = self.destination_directory.split("/")[-1]
                    # shard_name = (
                    #     f"{dest_base}-{split_name}.tfrecord-{i:05d}-of-{num_shards:05d}"
                    # )

                    # Write single shard
                    bytes = self._write_shard((shard_info, shard_name), verbose=verbose)

                    # Get bytes information
                    split_bytes += bytes

            # Create split information
            split_infos.append(
                tfds.core.SplitInfo(
                    name=split_name,
                    shard_lengths=[len(shard) for shard in split["shards"]],
                    num_bytes=split_bytes,
                    # filename_template="{DATASET}-{SPLIT}.{FILEFORMAT}-{SHARD_X_OF_Y}",
                )
            )

        self._write_meta_data(split_infos)

    def _create_shards(self, info, examples_per_shard):
        n_shards = math.ceil(len(info) / examples_per_shard)

        # Split into shards
        split_shards = np.array_split(info, n_shards)

        return split_shards

    def write_records(
        self,
        splits_info=None,
        splits_shards=None,
        shuffle=True,
        random_seed=42,
        examples_per_shard=None,
        mb_per_shard=None,
        n_estimates_mb_per_example=1,
        n_estimates_mb_per_split_example=None,
        n_workers=1,
        verbose=0,
    ):
        """Writes data into a TFRecord format.

        Parameters
        ----------
        splits_info : list, optional
            List of dictionaries containing information to be iterated over, by default None
        splits_shards : list, optional
            List of precomputed shards to be saved and iterated over, by default None
        shuffle : bool, optional
            Whether or not to shuffle the data provided, by default True
        random_seed : int, optional
            Random seed provided to shuffle for reproducibility, by default 42
        examples_per_shard : int, optional
            Number of examples per shard, by default None
        mb_per_shard : number, optional
            Allow writer to estimate the number of examples per shard with a target memory usage in MB for each shard, by default None
        n_estimates_mb_per_example : int, optional
            Number of estimates to compute the number of examples per shard if mb_per_shard is provided, by default 1
        n_estimates_mb_per_split_example : dict, optional
            Similar to n_estimates_mb_per_example but for each individual split, by default None
        n_workers : int, optional
            Number of workers to use for multiprocessing, by default 1
        verbose : int, optional
            How much logging information to display.  Values are 0 (silent), 1, 2, 3 (most), by default 0
        """

        # Validate parameters
        if splits_info is None and splits_shards is None:
            raise ValueError(
                f"Information or shards must be supplied for at least one split."
            )

        if splits_info is not None and splits_shards is None:
            contains_info, split_names = zip(
                *[(split["info"] is not None, split["name"]) for split in splits_info]
            )

            if not any(contains_info):
                raise ValueError(
                    f"Information must be supplied for at least one of {' ,'.join(split_names)}."
                )

        if splits_info is None and splits_shards is not None:
            contains_shards, split_names = zip(
                *[(split["shards"] is not None, split["name"]) for split in splits_info]
            )

            if not any(contains_shards):
                raise ValueError(
                    f"Information must be supplied for at least one of {' ,'.join(split_names)}."
                )

            if mb_per_shard is not None:
                raise ValueError(f"If shards are supplied, mb_per_shard must be None.")

        if mb_per_shard is not None:
            if mb_per_shard <= 0:
                raise ValueError(f"mb_per_shard must be > 0.")

            if n_estimates_mb_per_split_example is not None:
                if n_estimates_mb_per_example is not None:
                    warnings.warn(
                        "Ignoring n_estimates_mb_per_example as n_estimates_mb_per_split_example is not None.  This may not be desired behavior."
                    )

                ...

        # Setup shards
        if splits_shards is not None:
            if shuffle:
                for split_shard in splits_shards:
                    split_shard["shards"] = sklearn_shuffle(
                        split_shard["shards"], random_state=random_seed
                    )

        # Setup shards
        if splits_info is not None:
            splits_shards = []
            for split in splits_info:
                split_shard_info = {}
                split_name = split["name"]
                split_info = split["info"]

                if shuffle:
                    split_info = sklearn_shuffle(split_info, random_state=random_seed)

                if examples_per_shard is not None:
                    split_examples_per_shard = examples_per_shard

                else:
                    split_examples_per_shard = split.get("examples_per_shard")

                if split_examples_per_shard is None and mb_per_shard is None:
                    raise ValueError(
                        f"Either mb_per_shard must be specified or examples_per_shard must be specified in splits_info for {split_name} split."
                    )
                if split_examples_per_shard is not None and mb_per_shard is not None:
                    warnings.warn(
                        f"Ignoring mb_per_shard as split_examples_per_shard is not None for {split_name} split.  This may not be desired behavior."
                    )

                # If compute elements per shard based on memory
                if split_examples_per_shard is None and mb_per_shard is not None:
                    # Estimate for each split
                    n_estimates = None
                    if n_estimates_mb_per_split_example is not None:
                        n_estimates = n_estimates_mb_per_split_example.get(split_name)
                        if (
                            n_estimates is not None
                            and n_estimates_mb_per_example is not None
                        ):
                            warnings.warn(
                                f"Ignoring n_estimates_mb_per_example as n_estimates_mb_per_split_example is not None for {split_name} split.  This may not be desired behavior."
                            )
                    else:
                        n_estimates = n_estimates_mb_per_example

                    validate_n_estimates(n_estimates, f"  For {split_name} split.")

                    split_examples_per_shard = self._estimate_n_examples_per_shard(
                        mb_per_shard,
                        split_info,
                        n_estimates_mb_per_example,
                        verbose=verbose,
                    )

                # Instead of splitting into shards, it may be better to just compute the
                # number of elements in each shard.  This would make the code more
                # compatible with tf.data.Datasets.

                # Split the info into shards
                split_shard_info["shards"] = self._create_shards(
                    split_info, split_examples_per_shard
                )

                if verbose > 0:
                    n_shards = len(split_shard_info["shards"])

                    # Convert MB to human readable form in GB/MB
                    total_mb = n_shards * mb_per_shard
                    termcolor.cprint(
                        termcolor.colored(
                            f"Computed {n_shards} shards for {split_name} split.  Estimate: {convert_to_human_readable(total_mb)}",
                            "green",
                            attrs=["bold"],
                        )
                    )
                    # print(
                    #     f"Computed {n_shards} shards for {split_name} split.  Estimate: {convert_to_human_readable(total_mb)}"
                    # )

                # Populate splits_shards so they can be written
                split_shard_info["name"] = split_name

                splits_shards.append(split_shard_info)

        # Write shards
        self._write_shards(splits_shards, verbose=verbose)

        # Write extended dataset info
        self._write_extended_dataset_info()


def validate_n_estimates(n_estimates, err=""):
    if not isinstance(n_estimates, int):
        err = "Number of estimates must be an integer." + err
        raise ValueError(err)


def check_parameters(param1, param2):
    if (param1 is not None and param2 is None) or (
        param1 is None and param2 is not None
    ):
        # One parameter is not None and the other is None
        return False
    return True
